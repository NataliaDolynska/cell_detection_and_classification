{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSGc6fW7zC8c"
      },
      "source": [
        "# Preparation for Milestone Four\n",
        "\n",
        "Today, we will begin preparing for the final milestone. Here, we will assemble all the pieces of the pipeline you've created. You will need to write a function **compute_AgNOR_score**. This function first utilizes the detection model to locate cells within a given image and then feeds those cells into a classification model to classify them into one of the AgNOR classes. Finally, you will aggregate all predictions into a final AgNOR score for the entire image."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.814093Z",
          "start_time": "2024-05-26T21:21:07.452158Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-fbh4A-zC8e",
        "outputId": "a438d6e0-625a-4723-9faf-b6fedf78c26b"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def pretty_print_title(title='', length=50):\n",
        "    if len(title.strip()) > 0:\n",
        "        length = length - len(title.strip()) - 2  # Calculate remaining space for padding\n",
        "        print(f'{\"*\" * round(length / 2)} {title} {\"*\" * round(length / 2)}')\n",
        "    else:\n",
        "        print(f'{\"*\" * length}')\n",
        "\n",
        "\n",
        "if sys.platform == 'darwin':\n",
        "    base_path = f'{str(Path.home())}/Projects/computer-vision-thi/dataset/AgNOR_Project'\n",
        "    !set VIRTUAL_ENV f\"{str(Path.home())}}/Projects/computer-vision-thi/.venv\"\n",
        "    device = 'mps'\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    # !pip install 'pandas<2.0.0'\n",
        "    # !pip install pillow\n",
        "    # !pip install tqdm\n",
        "    # !pip install albumentations\n",
        "    # !pip install tensorflow\n",
        "    # !pip install matplotlib\n",
        "    # !pip install torchmetrics\n",
        "    # !pip install imbalanced-learn\n",
        "    # !pip install opencv-python\n",
        "    import torch\n",
        "    base_path = '/content/gdrive/MyDrive/AgNORs'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "path_to_images = f'{base_path}/'\n",
        "path_to_annotations = f'{base_path}/annotation_frame.p'\n",
        "\n",
        "print(f'Path to images: {path_to_images}')\n",
        "print(f'Path to annotations: {path_to_annotations}')\n",
        "print(f'Device is: {device}')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Path to images: /content/gdrive/MyDrive/AgNORs/\n",
            "Path to annotations: /content/gdrive/MyDrive/AgNORs/annotation_frame.p\n",
            "Device is: cuda\n"
          ]
        }
      ],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "\n",
        "def get_less_used_gpu(gpus=None, debug=False):\n",
        "    \"\"\"Inspect cached/reserved and allocated memory on specified gpus and return the id of the less used device\"\"\"\n",
        "    if gpus is None:\n",
        "        warn = 'Falling back to default: all gpus'\n",
        "        gpus = range(cuda.device_count())\n",
        "    elif isinstance(gpus, str):\n",
        "        gpus = [int(el) for el in gpus.split(',')]\n",
        "\n",
        "    # check gpus arg VS available gpus\n",
        "    sys_gpus = list(range(cuda.device_count()))\n",
        "    if len(gpus) > len(sys_gpus):\n",
        "        gpus = sys_gpus\n",
        "        warn = f'WARNING: Specified {len(gpus)} gpus, but only {cuda.device_count()} available. Falling back to default: all gpus.\\nIDs:\\t{list(gpus)}'\n",
        "    elif set(gpus).difference(sys_gpus):\n",
        "        # take correctly specified and add as much bad specifications as unused system gpus\n",
        "        available_gpus = set(gpus).intersection(sys_gpus)\n",
        "        unavailable_gpus = set(gpus).difference(sys_gpus)\n",
        "        unused_gpus = set(sys_gpus).difference(gpus)\n",
        "        gpus = list(available_gpus) + list(unused_gpus)[:len(unavailable_gpus)]\n",
        "        warn = f'GPU ids {unavailable_gpus} not available. Falling back to {len(gpus)} device(s).\\nIDs:\\t{list(gpus)}'\n",
        "\n",
        "    cur_allocated_mem = {}\n",
        "    cur_cached_mem = {}\n",
        "    max_allocated_mem = {}\n",
        "    max_cached_mem = {}\n",
        "    for i in gpus:\n",
        "        cur_allocated_mem[i] = cuda.memory_allocated(i)\n",
        "        cur_cached_mem[i] = cuda.memory_reserved(i)\n",
        "        max_allocated_mem[i] = cuda.max_memory_allocated(i)\n",
        "        max_cached_mem[i] = cuda.max_memory_reserved(i)\n",
        "    min_allocated = min(cur_allocated_mem, key=cur_allocated_mem.get)\n",
        "    if debug:\n",
        "        print(warn)\n",
        "        print('Current allocated memory:', {f'cuda:{k}': v for k, v in cur_allocated_mem.items()})\n",
        "        print('Current reserved memory:', {f'cuda:{k}': v for k, v in cur_cached_mem.items()})\n",
        "        print('Maximum allocated memory:', {f'cuda:{k}': v for k, v in max_allocated_mem.items()})\n",
        "        print('Maximum reserved memory:', {f'cuda:{k}': v for k, v in max_cached_mem.items()})\n",
        "        print('Suggested GPU:', min_allocated)\n",
        "    return min_allocated\n",
        "\n",
        "\n",
        "def free_memory(to_delete: list, debug=False):\n",
        "    import gc\n",
        "    import inspect\n",
        "    calling_namespace = inspect.currentframe().f_back\n",
        "    if debug:\n",
        "        print('Before:')\n",
        "        get_less_used_gpu(debug=True)\n",
        "\n",
        "    for _var in to_delete:\n",
        "        calling_namespace.f_locals.pop(_var, None)\n",
        "        gc.collect()\n",
        "        cuda.empty_cache()\n",
        "    if debug:\n",
        "        print('After:')\n",
        "        get_less_used_gpu(debug=True)\n",
        "\n",
        "\n",
        "def wipe_memory(debug=True):\n",
        "    get_less_used_gpu(debug=debug)\n",
        "    free_memory(['cuda:0'], debug=debug)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # print(torch.cuda.set_per_process_memory_fraction(0.6)) release memory cashe\n",
        "\n"
      ],
      "metadata": {
        "id": "pK6AVwg3n4z9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.842866Z",
          "start_time": "2024-05-26T21:21:07.822613Z"
        },
        "id": "Ec3JVyozzC8e"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.transforms import functional as F\n",
        "import torchmetrics\n",
        "import albumentations as A\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "import torchvision\n",
        "from math import ceil"
      ],
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzUSIWCzC8d"
      },
      "source": [
        "# 1. Write a function \"process_image\" which receives an image and runs the detection model on it.\n",
        "\n",
        "The function should have the following parameters:\n",
        "\n",
        "1. image: The image on which you want to run inference.\n",
        "2. crop_size: The size of the crops you want to load from the image.\n",
        "3. overlap: Percentage or number of pixels the crops should overlap.\n",
        "4. model: The object detection model. This function should generally be able to run with any detection model.\n",
        "5. detection_threshold: A threshold to apply to the detections to reject false positives.\n",
        "\n",
        "The function will have to tile the image into **overlapping crops** and then feed each crop to the model. After that, all detections have to be transformed to the global coordinate system of the image since the detections are within the coordinate system of the image crop. Subsequently, [non-maximal suppression](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html) needs to be applied to the detections in order to reject overlapping detections. In the end, the function will return the coordinates and scores of the detected cells that exceed the given threshold. Use **torch_no_grad** to save computation time and also ensure your **model is in evaluation mode** before feeding the cells to it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def process_image(image, crop_size, overlap, model, detection_threshold):\n",
        "    \"\"\"\n",
        "    Runs the detection model on an image with overlapping crops.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The image on which to run inference (NumPy array).\n",
        "    - crop_size: The size of the crops (tuple of (height, width)).\n",
        "    - overlap: Percentage or number of pixels the crops should overlap (float or int).\n",
        "    - model: The object detection model.\n",
        "    - detection_threshold: Threshold to apply to the detections to reject false positives (float).\n",
        "\n",
        "    \"\"\"\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate stride based on overlap\n",
        "    stride_y = int(crop_size * (1 - overlap)) if isinstance(overlap, float) else crop_size - overlap\n",
        "    stride_x = int(crop_size * (1 - overlap)) if isinstance(overlap, float) else crop_size - overlap\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    detections = []\n",
        "\n",
        "    # Convert image to tensor # normalize the image according to previous\n",
        "    image_tensor = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "    image_tensor = torch.from_numpy(image_tensor).float().permute(2, 0, 1)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Loop over the image with overlapping crops\n",
        "        for y in range(0, height, stride_y):\n",
        "            for x in range(0, width, stride_x):\n",
        "                # Calculate crop boundaries\n",
        "                y1 = y\n",
        "                y2 = min(y + crop_size, height)\n",
        "                x1 = x\n",
        "                x2 = min(x + crop_size, width)\n",
        "\n",
        "                crop = image_tensor[:, y1:y2, x1:x2]\n",
        "                crop = crop.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "                # Run detection model on the crop\n",
        "                outputs = model(crop)[0]\n",
        "\n",
        "                # Filter out detections below the threshold\n",
        "                scores = outputs['scores']\n",
        "                boxes = outputs['boxes']\n",
        "                valid_indices = scores > detection_threshold\n",
        "                scores = scores[valid_indices]\n",
        "                boxes = boxes[valid_indices]\n",
        "\n",
        "                # Transform coordinates to the global image coordinate system\n",
        "                for box, score in zip(boxes, scores):\n",
        "                    x1_global = box[0] + x1\n",
        "                    y1_global = box[1] + y1\n",
        "                    x2_global = box[2] + x1\n",
        "                    y2_global = box[3] + y1\n",
        "                    detections.append((x1_global, y1_global, x2_global, y2_global, score.item()))\n",
        "\n",
        "    if not detections:\n",
        "        return []\n",
        "\n",
        "    # Convert detections to tensor for NMS\n",
        "    boxes = torch.tensor([det[:4] for det in detections])\n",
        "    scores = torch.tensor([det[4] for det in detections])\n",
        "\n",
        "    # Apply non-maximal suppression\n",
        "    nms_indices = nms(boxes, scores, iou_threshold=0.5)\n",
        "\n",
        "    final_detections = [detections[idx] for idx in nms_indices]\n",
        "\n",
        "    return final_detections"
      ],
      "metadata": {
        "id": "8jFtxkqbTsAu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvE6tkpzzC8e"
      },
      "source": [
        "# 2. Write a function \"process_cells\" which classifies the cells from the coordinates that were given to the model.\n",
        "\n",
        "The function should have the following parameters:\n",
        "\n",
        "1. image: The image from which to load the cells.\n",
        "2. coords: Coordinates of the cells which you found with the detection algorithm.\n",
        "3. model: The trained classification model.\n",
        "4. crop_size: A size to resize the crops to. It should be equal to the size with which you trained the classification network.\n",
        "\n",
        "The function should load each cell from the respective image and feed them to the classification model. Save the prediction and, in the end, aggregate the classifications of all cells into a final AgNOR score. The function should return the labels of the respective cells as well as the final AgNOR score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_cells(image, coords, model, crop_size):\n",
        "    \"\"\"\n",
        "    Classifies the cells from the given coordinates using the classification model.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The image from which to load the cells (NumPy array).\n",
        "    - coords: Coordinates of the cells which were found with the detection algorithm.\n",
        "    - model: The trained classification model.\n",
        "    - crop_size: A size to resize the crops to (tuple of (height, width)).\n",
        "\n",
        "    Returns:\n",
        "    - labels: List of labels for the respective cells.\n",
        "    - agnor_score: The aggregated AgNOR score based on the classifications.\n",
        "    Add normalization\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for coord in coords:\n",
        "            x1, y1, x2, y2, _ = coord\n",
        "            cell_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "            cell_image = Image.fromarray(cell_image)\n",
        "            cell_image = transform(cell_image).unsqueeze(0)\n",
        "            cell_image = cell_image.to(device)\n",
        "\n",
        "            output = model(cell_image)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            label = predicted.item()\n",
        "            labels.append(label)\n",
        "\n",
        "    return labels, np.mean(labels)"
      ],
      "metadata": {
        "id": "IpLXSRntWdcZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRs1bqoyzC8e"
      },
      "source": [
        "# 3. Combine both functions into the function **compute_AgNOR_score**.\n",
        "\n",
        "This function should receive the image as a parameter and also require all parameters to execute the subfunctions. In the end, this function should return the overall AgNOR score of the image."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.886601Z",
          "start_time": "2024-05-26T21:21:07.882336Z"
        },
        "id": "O_ygdqR2zC8f"
      },
      "cell_type": "code",
      "source": [
        "def compute_AgNOR_score(image, crop_size, overlap,resize, detection_model, detection_threshold, classification_model):\n",
        "    data = process_image(image, crop_size, overlap, detection_model, detection_threshold)\n",
        "\n",
        "    wipe_memory(False)\n",
        "\n",
        "    labels, AgNOR_score = process_cells(image, data, classification_model, resize)\n",
        "    return labels, AgNOR_score"
      ],
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHzy4j6XzC8f"
      },
      "source": [
        "# 4. Test your pipeline.\n",
        "\n",
        "Take several images (approximately 5) and run them through your pipeline. Then, calculate the error between the predicted AgNOR score and the AgNOR score defined by the labels of the cells in the annotation file. To obtain this label, simply calculate the mean of the labels of the respective image."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:23:15.995206Z",
          "start_time": "2024-05-26T21:23:15.787328Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dW5_jvczC8f",
        "outputId": "9f906780-09f7-45d2-f8da-ab8734f3f5b1"
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "wipe_memory(False)\n",
        "\n",
        "IMAGES_COUNT = 5\n",
        "\n",
        "annotation = pickle.load(open(path_to_annotations, 'rb'))\n",
        "print(annotation.head())\n",
        "\n",
        "annotation_file_names = annotation.filename.unique().tolist()\n",
        "\n",
        "test_images = random.sample(annotation_file_names, IMAGES_COUNT)\n",
        "\n",
        "\n",
        "print(\"Device\", device)\n",
        "detection_model = torch.load(\"/content/gdrive/MyDrive/models/detection_model.pth\", map_location=device)\n",
        "classification_model = torch.load(\"/content/gdrive/MyDrive/models/classification_model.pth\", map_location=device)\n",
        "\n",
        "\n",
        "detection_model.eval()\n",
        "classification_model.eval()\n",
        "detection_model.to(device)\n",
        "classification_model.to(device)\n",
        "\n",
        "crop_size = 250\n",
        "overlap = 0.2\n",
        "detection_threshold = 0.5\n",
        "resize = 100\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_name):\n",
        "        img = Image.open(os.path.join(path_to_images, image_name)).convert('RGB')\n",
        "        image = np.array(img)\n",
        "        return image\n",
        "\n",
        "for i, image_name in enumerate(test_images):\n",
        "\n",
        "    image = load_image(image_name)\n",
        "    # Run the image through the pipeline\n",
        "    labels, score =  compute_AgNOR_score(image, crop_size, overlap, resize, detection_model, detection_threshold, classification_model)\n",
        "    actual_score = annotation[annotation['filename'] == image_name]['label'].mean()\n",
        "    # Calculate the error\n",
        "    error = abs(score - actual_score)\n",
        "\n",
        "    print(f\"Image {i+1} ({image_name}): Predicted Score = {score}, Actual Score = {actual_score}, Error = {error}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          filename  max_x  max_y  min_x  min_y  label\n",
            "0  AgNOR_0495.tiff     26     41      4     15      1\n",
            "1  AgNOR_0495.tiff     71     23     42      0      2\n",
            "2  AgNOR_0495.tiff    133     61    104     37      1\n",
            "3  AgNOR_0495.tiff    143    117    121     88      2\n",
            "4  AgNOR_0495.tiff    224     37    199     12      1\n",
            "Device cuda\n",
            "Image 1 (AgNOR_2999.tiff): Predicted Score = 1.4814814814814814, Actual Score = 1.337573385518591, Error = 0.14390809596289045\n",
            "Image 2 (AgNOR_0495.tiff): Predicted Score = 1.736842105263158, Actual Score = 1.3578643578643579, Error = 0.3789777473988001\n",
            "Image 3 (AgNOR_2862.tiff): Predicted Score = 1.7246376811594204, Actual Score = 1.3300536672629697, Error = 0.3945840138964507\n",
            "Image 4 (AgNOR_0677.tiff): Predicted Score = 1.7272727272727273, Actual Score = 2.112208892025406, Error = 0.38493616475267856\n",
            "Image 5 (AgNOR_0531.tiff): Predicted Score = 1.5714285714285714, Actual Score = 1.0815286624203821, Error = 0.48989990900818925\n"
          ]
        }
      ],
      "execution_count": 41
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
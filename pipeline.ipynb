{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSGc6fW7zC8c"
      },
      "source": [
        "## Pipeline Overview\n",
        "The **compute_AgNOR_score** function processes an image by first detecting cells using a detection model. These detected cells are then classified into AgNOR categories using a classification model. Finally, the function aggregates the classification results to generate a comprehensive AgNOR score for the entire image."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.814093Z",
          "start_time": "2024-05-26T21:21:07.452158Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-fbh4A-zC8e",
        "outputId": "a438d6e0-625a-4723-9faf-b6fedf78c26b"
      },
      "cell_type": "code",
      "source": [
        "# Platform-specific environment setup and path configuration\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def pretty_print_title(title='', length=50):\n",
        "    if len(title.strip()) > 0:\n",
        "        length = length - len(title.strip()) - 2  # Calculate remaining space for padding\n",
        "        print(f'{\"*\" * round(length / 2)} {title} {\"*\" * round(length / 2)}')\n",
        "    else:\n",
        "        print(f'{\"*\" * length}')\n",
        "\n",
        "\n",
        "if sys.platform == 'darwin':\n",
        "    base_path = f'{str(Path.home())}/Projects/computer-vision-thi/dataset/AgNOR_Project'\n",
        "    !set VIRTUAL_ENV f\"{str(Path.home())}}/Projects/computer-vision-thi/.venv\"\n",
        "    device = 'mps'\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    # !pip install 'pandas<2.0.0'\n",
        "    # !pip install pillow\n",
        "    # !pip install tqdm\n",
        "    # !pip install albumentations\n",
        "    # !pip install tensorflow\n",
        "    # !pip install matplotlib\n",
        "    # !pip install torchmetrics\n",
        "    # !pip install imbalanced-learn\n",
        "    # !pip install opencv-python\n",
        "    import torch\n",
        "    base_path = '/content/gdrive/MyDrive/AgNORs'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "path_to_images = f'{base_path}/'\n",
        "path_to_annotations = f'{base_path}/annotation_frame.p'\n",
        "\n",
        "print(f'Path to images: {path_to_images}')\n",
        "print(f'Path to annotations: {path_to_annotations}')\n",
        "print(f'Device is: {device}')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Path to images: /content/gdrive/MyDrive/AgNORs/\n",
            "Path to annotations: /content/gdrive/MyDrive/AgNORs/annotation_frame.p\n",
            "Device is: cuda\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU memory management and cleanup utilities\n",
        "from torch import cuda\n",
        "\n",
        "\n",
        "def get_less_used_gpu(gpus=None, debug=False):\n",
        "    \"\"\"Inspect cached/reserved and allocated memory on specified gpus and return the id of the less used device\"\"\"\n",
        "    if gpus is None:\n",
        "        warn = 'Falling back to default: all gpus'\n",
        "        gpus = range(cuda.device_count())\n",
        "    elif isinstance(gpus, str):\n",
        "        gpus = [int(el) for el in gpus.split(',')]\n",
        "\n",
        "    # check gpus arg VS available gpus\n",
        "    sys_gpus = list(range(cuda.device_count()))\n",
        "    if len(gpus) > len(sys_gpus):\n",
        "        gpus = sys_gpus\n",
        "        warn = f'WARNING: Specified {len(gpus)} gpus, but only {cuda.device_count()} available. Falling back to default: all gpus.\\nIDs:\\t{list(gpus)}'\n",
        "    elif set(gpus).difference(sys_gpus):\n",
        "        # take correctly specified and add as much bad specifications as unused system gpus\n",
        "        available_gpus = set(gpus).intersection(sys_gpus)\n",
        "        unavailable_gpus = set(gpus).difference(sys_gpus)\n",
        "        unused_gpus = set(sys_gpus).difference(gpus)\n",
        "        gpus = list(available_gpus) + list(unused_gpus)[:len(unavailable_gpus)]\n",
        "        warn = f'GPU ids {unavailable_gpus} not available. Falling back to {len(gpus)} device(s).\\nIDs:\\t{list(gpus)}'\n",
        "\n",
        "    cur_allocated_mem = {}\n",
        "    cur_cached_mem = {}\n",
        "    max_allocated_mem = {}\n",
        "    max_cached_mem = {}\n",
        "    for i in gpus:\n",
        "        cur_allocated_mem[i] = cuda.memory_allocated(i)\n",
        "        cur_cached_mem[i] = cuda.memory_reserved(i)\n",
        "        max_allocated_mem[i] = cuda.max_memory_allocated(i)\n",
        "        max_cached_mem[i] = cuda.max_memory_reserved(i)\n",
        "    min_allocated = min(cur_allocated_mem, key=cur_allocated_mem.get)\n",
        "    if debug:\n",
        "        print(warn)\n",
        "        print('Current allocated memory:', {f'cuda:{k}': v for k, v in cur_allocated_mem.items()})\n",
        "        print('Current reserved memory:', {f'cuda:{k}': v for k, v in cur_cached_mem.items()})\n",
        "        print('Maximum allocated memory:', {f'cuda:{k}': v for k, v in max_allocated_mem.items()})\n",
        "        print('Maximum reserved memory:', {f'cuda:{k}': v for k, v in max_cached_mem.items()})\n",
        "        print('Suggested GPU:', min_allocated)\n",
        "    return min_allocated\n",
        "\n",
        "\n",
        "def free_memory(to_delete: list, debug=False):\n",
        "    import gc\n",
        "    import inspect\n",
        "    calling_namespace = inspect.currentframe().f_back\n",
        "    if debug:\n",
        "        print('Before:')\n",
        "        get_less_used_gpu(debug=True)\n",
        "\n",
        "    for _var in to_delete:\n",
        "        calling_namespace.f_locals.pop(_var, None)\n",
        "        gc.collect()\n",
        "        cuda.empty_cache()\n",
        "    if debug:\n",
        "        print('After:')\n",
        "        get_less_used_gpu(debug=True)\n",
        "\n",
        "def wipe_memory(debug=True):\n",
        "    get_less_used_gpu(debug=debug)\n",
        "    free_memory(['cuda:0'], debug=debug)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # print(torch.cuda.set_per_process_memory_fraction(0.6)) release memory cashe"
      ],
      "metadata": {
        "id": "pK6AVwg3n4z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.842866Z",
          "start_time": "2024-05-26T21:21:07.822613Z"
        },
        "id": "Ec3JVyozzC8e"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.transforms import functional as F\n",
        "import torchmetrics\n",
        "import albumentations as A\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "import torchvision\n",
        "from math import ceil"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzUSIWCzC8d"
      },
      "source": [
        "# Function **process_image**\n",
        "\n",
        "The **process_image** function runs object detection on an image by tiling it into overlapping crops, applying the detection model, and returning the detected coordinates and scores.\n",
        "\n",
        "Parameters:\n",
        "- image: Input image.\n",
        "\n",
        "- crop_size: Size of the crops.\n",
        "\n",
        "- overlap: Overlap between crops.\n",
        "\n",
        "- model: Object detection model.\n",
        "\n",
        "- detection_threshold: Threshold to filter low-confidence detections.\n",
        "\n",
        "The function processes each crop, transforms the coordinates to the global image, applies non-maximum suppression to remove duplicates, and returns the valid detections. torch.no_grad() and set the model used in evaluation mode for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def process_image(image, crop_size, overlap, model, detection_threshold):\n",
        "    \"\"\"\n",
        "    Runs the detection model on an image with overlapping crops.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The image on which to run inference (NumPy array).\n",
        "    - crop_size: The size of the crops (tuple of (height, width)).\n",
        "    - overlap: Percentage or number of pixels the crops should overlap (float or int).\n",
        "    - model: The object detection model.\n",
        "    - detection_threshold: Threshold to apply to the detections to reject false positives (float).\n",
        "\n",
        "    \"\"\"\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate stride based on overlap\n",
        "    stride_y = int(crop_size * (1 - overlap)) if isinstance(overlap, float) else crop_size - overlap\n",
        "    stride_x = int(crop_size * (1 - overlap)) if isinstance(overlap, float) else crop_size - overlap\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    detections = []\n",
        "\n",
        "    # Convert image to tensor # normalize the image according to previous\n",
        "    image_tensor = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "    image_tensor = torch.from_numpy(image_tensor).float().permute(2, 0, 1)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Loop over the image with overlapping crops\n",
        "        for y in range(0, height, stride_y):\n",
        "            for x in range(0, width, stride_x):\n",
        "                # Calculate crop boundaries\n",
        "                y1 = y\n",
        "                y2 = min(y + crop_size, height)\n",
        "                x1 = x\n",
        "                x2 = min(x + crop_size, width)\n",
        "\n",
        "                crop = image_tensor[:, y1:y2, x1:x2]\n",
        "                crop = crop.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "                # Run detection model on the crop\n",
        "                outputs = model(crop)[0]\n",
        "\n",
        "                # Filter out detections below the threshold\n",
        "                scores = outputs['scores']\n",
        "                boxes = outputs['boxes']\n",
        "                valid_indices = scores > detection_threshold\n",
        "                scores = scores[valid_indices]\n",
        "                boxes = boxes[valid_indices]\n",
        "\n",
        "                # Transform coordinates to the global image coordinate system\n",
        "                for box, score in zip(boxes, scores):\n",
        "                    x1_global = box[0] + x1\n",
        "                    y1_global = box[1] + y1\n",
        "                    x2_global = box[2] + x1\n",
        "                    y2_global = box[3] + y1\n",
        "                    detections.append((x1_global, y1_global, x2_global, y2_global, score.item()))\n",
        "\n",
        "    if not detections:\n",
        "        return []\n",
        "\n",
        "    # Convert detections to tensor for NMS\n",
        "    boxes = torch.tensor([det[:4] for det in detections])\n",
        "    scores = torch.tensor([det[4] for det in detections])\n",
        "\n",
        "    # Apply non-maximal suppression\n",
        "    nms_indices = nms(boxes, scores, iou_threshold=0.5)\n",
        "\n",
        "    final_detections = [detections[idx] for idx in nms_indices]\n",
        "\n",
        "    return final_detections"
      ],
      "metadata": {
        "id": "8jFtxkqbTsAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvE6tkpzzC8e"
      },
      "source": [
        "#  Implement a function \"process_cells\" to classify cells based on provided coordinates\n",
        "\n",
        "**process_cells** function classifies cells from an image using the given coordinates.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "- Image containing the cells\n",
        "\n",
        "- Detected cell coordinates\n",
        "\n",
        "- Trained classification model\n",
        "\n",
        "- Crop size (matching the model's training input size)\n",
        "\n",
        "The function:\n",
        "\n",
        "1. Extracts and resizes each cell\n",
        "\n",
        "2. Classifies cells using the trained model\n",
        "\n",
        "3. Saves predictions for each cell\n",
        "\n",
        "4. Calculates the final AgNOR score\n",
        "\n",
        "Returns:\n",
        "\n",
        "- Labels of the cells\n",
        "\n",
        "- Final AgNOR score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_cells(image, coords, model, crop_size):\n",
        "    \"\"\"\n",
        "    Classifies the cells from the given coordinates using the classification model.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The image from which to load the cells (NumPy array).\n",
        "    - coords: Coordinates of the cells which were found with the detection algorithm.\n",
        "    - model: The trained classification model.\n",
        "    - crop_size: A size to resize the crops to (tuple of (height, width)).\n",
        "\n",
        "    Returns:\n",
        "    - labels: List of labels for the respective cells.\n",
        "    - agnor_score: The aggregated AgNOR score based on the classifications.\n",
        "    Normalization added.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for coord in coords:\n",
        "            x1, y1, x2, y2, _ = coord\n",
        "            cell_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "            cell_image = Image.fromarray(cell_image)\n",
        "            cell_image = transform(cell_image).unsqueeze(0)\n",
        "            cell_image = cell_image.to(device)\n",
        "\n",
        "            output = model(cell_image)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            label = predicted.item()\n",
        "            labels.append(label)\n",
        "\n",
        "    return labels, np.mean(labels)"
      ],
      "metadata": {
        "id": "IpLXSRntWdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRs1bqoyzC8e"
      },
      "source": [
        "# Refactor both functions into a single function called **compute_AgNOR_score**\n",
        "\n",
        "Function receive the image as a parameter and also require all parameters to execute the subfunctions. In the end, this function return the overall AgNOR score of the image."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:21:07.886601Z",
          "start_time": "2024-05-26T21:21:07.882336Z"
        },
        "id": "O_ygdqR2zC8f"
      },
      "cell_type": "code",
      "source": [
        "def compute_AgNOR_score(image, crop_size, overlap,resize, detection_model, detection_threshold, classification_model):\n",
        "    data = process_image(image, crop_size, overlap, detection_model, detection_threshold)\n",
        "\n",
        "    wipe_memory(False)\n",
        "\n",
        "    labels, AgNOR_score = process_cells(image, data, classification_model, resize)\n",
        "    return labels, AgNOR_score"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHzy4j6XzC8f"
      },
      "source": [
        "# Testing of pipeline\n",
        "\n",
        "5 images were taken and processed through the pipeline. The error between the predicted AgNOR score and the AgNOR score, as defined by the labels in the annotation file, was then calculated. To obtain this label, the mean of the labels for the respective image was computed."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-26T21:23:15.995206Z",
          "start_time": "2024-05-26T21:23:15.787328Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dW5_jvczC8f",
        "outputId": "9f906780-09f7-45d2-f8da-ab8734f3f5b1"
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "wipe_memory(False)\n",
        "\n",
        "IMAGES_COUNT = 5\n",
        "\n",
        "annotation = pickle.load(open(path_to_annotations, 'rb'))\n",
        "print(annotation.head())\n",
        "\n",
        "annotation_file_names = annotation.filename.unique().tolist()\n",
        "\n",
        "test_images = random.sample(annotation_file_names, IMAGES_COUNT)\n",
        "\n",
        "\n",
        "print(\"Device\", device)\n",
        "detection_model = torch.load(\"/content/gdrive/MyDrive/models/detection_model.pth\", map_location=device)\n",
        "classification_model = torch.load(\"/content/gdrive/MyDrive/models/classification_model.pth\", map_location=device)\n",
        "\n",
        "\n",
        "detection_model.eval()\n",
        "classification_model.eval()\n",
        "detection_model.to(device)\n",
        "classification_model.to(device)\n",
        "\n",
        "crop_size = 250\n",
        "overlap = 0.2\n",
        "detection_threshold = 0.5\n",
        "resize = 100\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_name):\n",
        "        img = Image.open(os.path.join(path_to_images, image_name)).convert('RGB')\n",
        "        image = np.array(img)\n",
        "        return image\n",
        "\n",
        "for i, image_name in enumerate(test_images):\n",
        "\n",
        "    image = load_image(image_name)\n",
        "    # Run the image through the pipeline\n",
        "    labels, score =  compute_AgNOR_score(image, crop_size, overlap, resize, detection_model, detection_threshold, classification_model)\n",
        "    actual_score = annotation[annotation['filename'] == image_name]['label'].mean()\n",
        "    # Calculate the error\n",
        "    error = abs(score - actual_score)\n",
        "\n",
        "    print(f\"Image {i+1} ({image_name}): Predicted Score = {score}, Actual Score = {actual_score}, Error = {error}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          filename  max_x  max_y  min_x  min_y  label\n",
            "0  AgNOR_0495.tiff     26     41      4     15      1\n",
            "1  AgNOR_0495.tiff     71     23     42      0      2\n",
            "2  AgNOR_0495.tiff    133     61    104     37      1\n",
            "3  AgNOR_0495.tiff    143    117    121     88      2\n",
            "4  AgNOR_0495.tiff    224     37    199     12      1\n",
            "Device cuda\n",
            "Image 1 (AgNOR_2999.tiff): Predicted Score = 1.4814814814814814, Actual Score = 1.337573385518591, Error = 0.14390809596289045\n",
            "Image 2 (AgNOR_0495.tiff): Predicted Score = 1.736842105263158, Actual Score = 1.3578643578643579, Error = 0.3789777473988001\n",
            "Image 3 (AgNOR_2862.tiff): Predicted Score = 1.7246376811594204, Actual Score = 1.3300536672629697, Error = 0.3945840138964507\n",
            "Image 4 (AgNOR_0677.tiff): Predicted Score = 1.7272727272727273, Actual Score = 2.112208892025406, Error = 0.38493616475267856\n",
            "Image 5 (AgNOR_0531.tiff): Predicted Score = 1.5714285714285714, Actual Score = 1.0815286624203821, Error = 0.48989990900818925\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}